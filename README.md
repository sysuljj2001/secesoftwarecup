# 适用于多智能体动态任务点的 Q-learning 算法

## 单智能体强化学习

### 状态描述

一个状态包含以下信息：

- 机器人携带物品状态 $\xi$（0为不携带 1-8为携带）
- 机器人能够最快到达的 $n$ 个工作台，不考虑 id，只考虑种类
- 这 $n$ 个工作台的状态
  - 工作台种类 $s_c$
  - 工作台产品格 $s_p$
  - 工作台原材料格 $s_r$
  - 工作台剩余生产时间 $s_l$（离散化为 $k$ 段）



### 状态转移

#### 训练过程

- 动作

  - 卖出转移：更新携带物品状态，更新机器人在卖出点最快到达的 $n$ 个工作台，即**随机生成 $n$ 个工作台，同时随机化它们的状态，作为新的 $n$ 个最近工作台**

  - 买入转移：类似上面

  - 销毁转移：更新携带物品状态，其它不变
- 奖励函数
  - 完成任务的时间 $E_t = \tau(t), t = [1, n]$，其中 $t$ 为机器人到达的工作台按时间排序
  - 任务成功奖励 $E_k = \mu(s_c,s_l)$
  - 买入的估计收益 $E_b = \eta(\xi, s_s, s_c, s_r)$
  - 卖出的估计收益 $E_s = \lambda(\xi, s_l, s_c, s_l)$



#### 推理过程

- 动作
  - 更新携带物品状态，根据实际地图计算机器人在新位置（也是某个工作台）的 $n$ 个最近工作台



## 全局强化学习

除了每个机器人的个体 q 表 $q^{(i)}$，在全局上也定义一个 q 表 $Q$

### 状态描述

- 4 个机器人各自执行的决策（例，{1 机器人} 前往 {种类 1} 工作台 {卖出}）
- 4个机器人两两之间的任务冲突
  - 哪两个机器人（6 种情况）
  - 是否是同一个任务（基于上一点）
    - 0 不是同种类任务 
    - 1 是同种类任务 
    - 2 同一个任务（在同一个工作台执行同种类任务）
- 不计入状态矩阵但是可观测的状态
  - 4 个机器人的 $n$ 个可选的工作台和它们的状态



### 状态转移

#### 训练过程

- 动作
  - 任意修改**一个**机器人的决策（例，{1 机器人} 前往 {种类 2} 工作台 {买入}），更新这个机器人的决策，更新机器人两两之间的任务冲突（原先是否有冲突、修改后是否冲突）
- 奖励函数
  - 修改决策后，观察原冲突是否解决、是否产生新冲突
    - 原冲突解决
      - 1类冲突：替换决策的代价，很可能为负
      - 2类冲突：只增加新决策的收益，一定为正
    - 新冲突
      - 1类冲突：对该决策收益作用一惩罚系数
      - 2类冲突：减去原决策收益
    - 具体的新收益计算与单体 q-learning 类似

- 反馈更新
  - 利用当前状态 $i$ 对应的每个机器人决策 $j$ 的 $Q_{ij}$ 更新对应的 $q^{(k)}_i, k =[1,4]$
    - 若全局认为需要改变决策 $j$ 为 $j'$，则降低 $q^{(i)}_{ij}$ ，增加 $q_{ij'}^{(k)}$
    - 若全局认为不需要改变决策 $j$，则增加 $q_{ij}^{(k)}$

#### 推理过程

- 动作
  - 与训练过程相同，但可修改决策的集合由具体条件（4 个机器人的 $n$ 个可选的工作台和它们的状态）约束
- 额外信息
  - 每次观测需要记录 4 个机器人的 $n$ 个可选的工作台和它们的状态



## 多智能体重规划

由于时间限制，全局 q 表不可能优化当前决策至最优收敛，故最后考虑规则式解决最终冲突。如果多个智能体选到同一个工作台执行同一任务，或有极大碰撞风险，考虑重规划任务。重规划是在线的，每帧都要执行。

考虑邻接的智能体 $i, j$ 之间可以相互传达信息，设置规则如下

- 若 $i, j$ 任务相同，则更新其中价值较小的对应的决策为其Q表中的另一最优决策
- 若 $i, j$ 任务不同，则根据距离判断是否需要将运动控制权交给避碰算法
- 若某个 $i$ 或 $j$ 无法更新决策，销毁物品

继续考虑所有工作台，若某个工作台收到了相同的任务请求，按类似方法更新决策。